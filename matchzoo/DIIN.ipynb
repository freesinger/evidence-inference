{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import matchzoo as mz\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mz.__version__)\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# nltk.set_proxy('SYSTEM PROXY')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = 'classification'\n",
    "\n",
    "classification_task = mz.tasks.Classification(num_classes=3)\n",
    "classification_task.metrics = ['acc']\n",
    "print(classification_task.num_classes)\n",
    "print(classification_task.output_shape)\n",
    "print(classification_task.output_dtype)\n",
    "print(classification_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pack Sample"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pack\n",
    "df = pd.DataFrame(data={'text_left': list('ABBCD'),\n",
    "                       'text_right': list('aacbd'),\n",
    "                       'label': [-1, 1, 0, 0, 1]})\n",
    "mz.pack(df, task=TYPE).frame()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pack\n",
    "left = [\n",
    "    ['artid1', 'A1'],\n",
    "    ['artid2', 'A2'],\n",
    "    ['artid3', 'A3']\n",
    "]\n",
    "right = [\n",
    "    ['hypoid1', 'prompt1'],\n",
    "    ['hypoid2', 'prompt2'],\n",
    "    ['hypoid3', 'prompt3']\n",
    "]\n",
    "relation = [\n",
    "    ['artid1', 'hypoid1', -1],\n",
    "    ['artid1', 'hypoid3', 1],\n",
    "    ['artid2', 'hypoid2', 0],\n",
    "    ['artid3', 'hypoid3', 1]\n",
    "]\n",
    "\n",
    "relation_df = pd.DataFrame(relation)\n",
    "# relation_df\n",
    "left = pd.DataFrame(left)\n",
    "right = pd.DataFrame(right)\n",
    "dp = mz.DataPack(\n",
    "    relation=relation_df,\n",
    "    left=left,\n",
    "    right=right\n",
    ")\n",
    "# print(len(dp))\n",
    "# print(type(dp.frame))\n",
    "# frame_slice = dp.frame[0:5]\n",
    "# type(frame_slice)\n",
    "# list(frame_slice.columns)\n",
    "# full_frame = dp.frame()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pack = mz.datasets.toy.load_data(stage='train')\n",
    "type(data_pack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot = pd.read_csv('../annotations/annotations_merged.csv')\n",
    "print(annot.dtypes)\n",
    "annot.sort_values('PMCID').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Process txt file for Articles input\"\"\"\n",
    "TXT_PATH = '../annotations/txt_files/'\n",
    "TAR_PATH = '../annotations/processed_txt_files/'\n",
    "look_up = {}\n",
    "\n",
    "if not os.path.exists(TAR_PATH):\n",
    "    os.mkdir(TAR_PATH)\n",
    "# else: os.removedirs(TAR_PATH)\n",
    "for file in os.listdir(TXT_PATH):\n",
    "    fname = file[3:]\n",
    "    look_up[int(file[3:-4])] = fname\n",
    "    with open(TXT_PATH+file, 'r', encoding='utf-8') as f, open(TAR_PATH+fname, 'w', encoding='utf-8') as t:\n",
    "        for line in f.readlines():\n",
    "            t.write(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Articles Map\"\"\"\n",
    "left_articles = []\n",
    "for file in os.listdir(TAR_PATH):\n",
    "    with open(TAR_PATH+file, 'r', encoding='utf-8') as f:\n",
    "        left_articles.append([int(file[:-4]), f.readlines()[0]])\n",
    "# print(left_articles[0:5])\n",
    "\n",
    "\"\"\"Prompts Map\"\"\"\n",
    "right_prompts = [list(pair) for pair in zip(annot['PromptID'].values, annot['Annotations'].values)]\n",
    "print(right_prompts[0:5])\n",
    "\n",
    "\"\"\"Articles <-> Prompts Map\"\"\"\n",
    "article_prompt_relations = [list(triplet) for triplet in zip(annot['PMCID'].values, \n",
    "                                                             annot['PromptID'].values, \n",
    "                                                             annot['Label Code'].values)]\n",
    "print(article_prompt_relations[0:2])\n",
    "\n",
    "\"\"\"Create Data-pack\"\"\"\n",
    "left = pd.DataFrame(left_articles, columns=['id_left', 'text_left'])\n",
    "right = pd.DataFrame(right_prompts, columns=['id_right', 'text_right'])\n",
    "relation = pd.DataFrame(article_prompt_relations, columns=['id_left', 'id_right', 'label'])\n",
    "dp = mz.DataPack(\n",
    "    relation = relation,\n",
    "    left = left,\n",
    "    right = right\n",
    ")\n",
    "\n",
    "# print(left)\n",
    "# print(len(dp))\n",
    "# print(type(dp.frame))\n",
    "# frame = dp.frame\n",
    "# print(list(frame().columns))\n",
    "# # frame_slice = dp.frame[0:5]\n",
    "# # type(frame_slice)\n",
    "# # list(frame_slice.columns)\n",
    "# full_frame = dp.frame()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Read article by name \"\"\"\n",
    "def read_article(id) -> str:\n",
    "    with open(TAR_PATH+look_up[id], 'r', encoding='utf-8') as f:\n",
    "        return f.readlines()[0]\n",
    "\n",
    "text_left = [read_article(id) for id in annot['PMCID']]\n",
    "# print(text_left[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\n",
    "    'id_left': annot['PMCID'].astype(str),\n",
    "    'text_left': text_left,\n",
    "    'id_right': annot['PromptID'].astype(str),\n",
    "    'text_right': annot['Annotations'],\n",
    "    'label': annot['Label Code']+1\n",
    "})\n",
    "print(df.dtypes)\n",
    "\n",
    "\"\"\" Split data pack into train/valid \"\"\"\n",
    "train, valid = train_test_split(df, test_size=0.2)\n",
    "train_pack = mz.pack(train, task=TYPE)\n",
    "valid_pack = mz.pack(valid, task=TYPE)\n",
    "train_pack.frame().head(10) # DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = mz.pack(df, task=TYPE)\n",
    "print(type(dp.frame))\n",
    "frame_slice = dp.frame[0:5]\n",
    "print(list(frame_slice.columns))\n",
    "full_frame = dp.frame()\n",
    "len(full_frame) == len(dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Train "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = mz.models.DIIN.get_default_preprocessor()\n",
    "train_processed = preprocessor.fit_transform(train_pack)\n",
    "valid_processed = preprocessor.transform(valid_pack)\n",
    "\n",
    "print(preprocessor.context)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_callback = mz.dataloader.callbacks.Ngram(preprocessor, mode='index')\n",
    "\n",
    "trainset = mz.dataloader.Dataset(\n",
    "    data_pack=train_processed,\n",
    "    mode='pair',\n",
    "    num_dup=1,\n",
    "    num_neg=4,\n",
    "    callbacks=[ngram_callback]\n",
    ")\n",
    "validset = mz.dataloader.Dataset(\n",
    "    data_pack=valid_processed,\n",
    "    mode='point',\n",
    "    callbacks=[ngram_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_callback = mz.models.DIIN.get_default_padding_callback()\n",
    "\n",
    "trainloader = mz.dataloader.DataLoader(\n",
    "    dataset=trainset,\n",
    "    stage='train',\n",
    "    callback=padding_callback\n",
    ")\n",
    "validloader = mz.dataloader.DataLoader(\n",
    "    dataset=validset,\n",
    "    stage='dev',\n",
    "    callback=padding_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mz.models.DIIN()\n",
    "model.params['task'] = classification_task\n",
    "model.params['embedding_output_dim'] = 100\n",
    "model.params['embedding_input_dim'] = preprocessor.context['embedding_input_dim']\n",
    "model.params['dropout_rate'] = 0.2\n",
    "model.guess_and_fill_missing_params()\n",
    "model.build()\n",
    "\n",
    "print(model)\n",
    "print('Trainable params: ', sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "trainer = mz.trainers.Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    trainloader=trainloader,\n",
    "    validloader=validloader,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}